{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215d246",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs178_sp25_hw2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7d466",
   "metadata": {
    "id": "43b7d466"
   },
   "source": [
    "<div align=\"center\">\n",
    "<h1> CS178: Machine Learning & Data Mining </h1>\n",
    "<h2> Homework 2: Due Friday 25 April 2025 (11:59 PM) </h2>\n",
    "<h3> Version 1.0 (Last Modified: 11 April 2025) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0921f03",
   "metadata": {
    "id": "a0921f03"
   },
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "This homework (and many subsequent ones) will involve data analysis and reporting on methods and results using Python code. You will submit **hw2.ipynb** that contains everything to Gradescope. Please do not rename the notebook. This includes any text you wish to include to describe your results, the complete code snippets of how you attempted each problem, and any figures that were generated (make sure to run all cells before submitting). It is important that you include enough detail that we know how you solved the problem, since otherwise we will be unable to grade it.\n",
    "\n",
    "Your homework will be given to you as a zipfile containing the data and a Jupyter notebook with problem descriptions and some template code that will help you get started. You **must** use this starter Jupyter notebook to complete your assignment.\n",
    "\n",
    "If you have any questions/concerns about using Jupyter notebooks, ask us on EdD.\n",
    "\n",
    "### Summary of Assignment: 100 total points\n",
    "- Problem 1: Gradient Descent (40 points)\n",
    "    - Problem 1.1: Visualize decision boundary of linear classifier (5 points)\n",
    "    - Problem 1.2: Implement a function to compute mean squared error (MSE) (5 points)\n",
    "    - Problem 1.3: Write out the gradient of MSE. They then implement a function that computes this gradient (10 points)\n",
    "    - Problem 1.4: Have the students implement gradient descent with the gradient they compute above. Have them create a plot of loss vs training iteration, to see it converges (10 points)\n",
    "    - Problem 1.5: Plot the trajectory of gradient descent for a few steps. Describe in words what they see happening (10 points)\n",
    "\n",
    "- Problem 2: Logistic Regression (55 points)\n",
    "    - Problem 2.1: Test logistic regression model implemented in sklearn (15 points)\n",
    "    - Problem 2.2: Explore Amazon Review dataset (10 points)\n",
    "    - Problem 2.3: Fit a logistic regression classifier on Amazon Review dataset. (10 points)\n",
    "    - Problem 2.4: Find the top k most negative/most positive features and corresponding coefficients. (10 points)\n",
    "    - Problem 2.5: Tune regularization parameter on Amazon Review dataset (10 points)\n",
    "- Statement of Collaboration (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cdb14",
   "metadata": {
    "id": "648cdb14"
   },
   "source": [
    "Before we get started, let's import some libraries that you will make use of in this assignment. Make sure that you run the code cell below in order to import these libraries.\n",
    "\n",
    "**Important: In the code block below, we set `seed=1234`. This is to ensure your code has reproducible results and is important for grading. Do not change this.**\n",
    "\n",
    "**Important: Do not change any codes we give you below, except for those waiting for you to complete. This is to ensure your code has reproducible results and is important for grading. Please do not delete/edit any otter grader cells or html tags identifying questions, otherwise you will not receive credit from the Gradescope autograder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788ad34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f788ad34",
    "outputId": "e6f1442e-c54d-4eaf-b2e3-90ca864d2d3f"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532bb26",
   "metadata": {
    "id": "2532bb26"
   },
   "source": [
    "---\n",
    "## Problem 1: Gradient Descent\n",
    "\n",
    "In this problem, you will implement gradient descent to train a linear model without a bias unit on a 2D Dataset by using the mean square error as loss function. **This linear model can be written as $f_\\theta(\\textbf{x}) = \\theta^T \\textbf{x} = \\theta_1 x_1 + \\theta_2 x_2$, with zero bias (i.e., $\\theta_0=0$). In this problem, $\\theta_0$ is ommited for simplicity and only $\\theta_1$ and $\\theta_2$ are optimized via gradient descent.**\n",
    "\n",
    "Let's first load in the dataset by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb808df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the seeds dataset\n",
    "seeds_file = \"data/Seed_Data.csv\"\n",
    "seeds_df = pd.read_csv(seeds_file)\n",
    "seeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da02f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the unique values of target variable\n",
    "target_values = seeds_df['target'].unique()\n",
    "target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75047c9",
   "metadata": {},
   "source": [
    "The dataset is comprised of measurements of geometrical properties of kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian represented by 70 elements each, with attributes derived from high quality visualizations of the internal kernel structure which was detected using a soft X-ray technique from the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. While the dataset includes 3 kernel varieties we will only look at two: Kama and Rosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'current_column_name' with the actual current column names in your DataFrame\n",
    "# to make the feature names more descriptive.\n",
    "seeds_df = seeds_df.rename(columns={\n",
    "    'A': 'area_A',\n",
    "    'P': 'perimeter_P',\n",
    "    'C': 'compactness_C',\n",
    "    'LK': 'length_of_kernel',\n",
    "    'WK': 'width_of_kernel',\n",
    "    'A_Coef': 'asymmetry_coefficient',\n",
    "    'LKG': 'length_of_kernel_groove'\n",
    "})\n",
    "\n",
    "seeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de64aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at 2 classes of target = 0 corresponding to Kama wheat and target = 1 \n",
    "# corresponding to Rosa wheat.\n",
    "seeds_subset = seeds_df[(seeds_df['target'] == 0) | (seeds_df['target'] == 1)]\n",
    "seeds_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our intuition, we will look at two features of the dataset across\n",
    "# the two targets we are interested in predicting.\n",
    "seeds_X = seeds_subset[['area_A', 'asymmetry_coefficient']]\n",
    "seeds_y = seeds_subset['target']\n",
    "X_1 = StandardScaler().fit_transform(seeds_X)     # scale the feature values (usually good to do this)\n",
    "y_1 = seeds_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb131ef",
   "metadata": {
    "id": "1eb131ef"
   },
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    # Make a figure with 1 subplot\n",
    "    fig, axes = plt.subplots()\n",
    "\n",
    "    # Scatter plot of features in X\n",
    "    feature_1 = X[:, 0]  # First column is the first feature (area_A)\n",
    "    feature_2 = X[:, 1]  # Second column is the second feature (asymmetry_coefficient)\n",
    "\n",
    "    axes.scatter(feature_1, feature_2, c=y)\n",
    "\n",
    "    axes.set_xlabel('area_A', fontsize=14)\n",
    "    axes.set_ylabel('asymmetry_coefficient', fontsize=14)\n",
    "    axes.set_title('Seed Area vs Asymmetry', fontsize=18);\n",
    "    \n",
    "    axes.set_xlim(-3,3)\n",
    "    axes.set_ylim(-3,3)\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1e0df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "d9a1e0df",
    "outputId": "d298c6b1-4055-4a24-c12b-2131e3e6d64a"
   },
   "outputs": [],
   "source": [
    "# Run the code below to illustrate the distribution of data points.\n",
    "fig, axes = plot_data(X_1, y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7031ca",
   "metadata": {
    "id": "4f7031ca"
   },
   "source": [
    "For this problem, we will focus on the properties of the gradient descent algorithm, rather than classification\n",
    "performance. Thus we will not create a separate validation dataset, and simply use all available data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f5f13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "100f5f13"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 1.1 (5 points): Visualize decision boundary of linear classifier\n",
    "\n",
    "In our code, the variable `X_1` is a numpy array containing the feature vectors in this 2D dataset, and `y_1` is a numpy array containing the corresponding labels, which are either -1 or 1.\n",
    "\n",
    "We have a linear model $f_\\theta(\\textbf{x}) = \\theta^T \\textbf{x} = \\theta_1 x_1 + \\theta_2 x_2$, with zero bias (i.e., $\\theta_0=0$), where $\\textbf{x}$ is an feature vector and $\\textbf{x}_j$ refers to the value of the j-th component of the vector $\\textbf{x}$. \n",
    "\n",
    "- Create a function that, given feature vectors `X`, labels `y` and parameters `theta_1`, `theta_2`, plots the feature vectors colored as their classes and the decision boundary corresponding to this linear classifier. \n",
    "\n",
    "Keep in mind that the points **do not** affect the linear classifier yet, we are only plotting the line in order to visualize it alongside the points. (Hint: The decision boundary is given by $\\theta_1 x_1 + \\theta_2 x_2 = 0$, consider `axes.plot` to draw this function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc21d76",
   "metadata": {
    "id": "1fc21d76",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_boundary(X, y, theta_1, theta_2): \n",
    "    '''\n",
    "    X:       [# feature vectors, # features], feature vectors\n",
    "    y:       [# feature vectors], labels\n",
    "    theta_1: the first parameter of linear model\n",
    "    theta_2: the second parameter of linear model\n",
    "    '''\n",
    "    \n",
    "    fig, axes = plot_data(X, y)\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    #since the range of the first feature of X, i.e. x1, is [-3, 3] (see the above plot), so we use x1b to denote the x-axis\n",
    "    ...\n",
    "    ###  YOUR CODE ENDS HERE  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac59010",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "deletable": false,
    "editable": false,
    "id": "dac59010",
    "outputId": "367e8266-0994-4506-ff1e-e92a33064ab5"
   },
   "outputs": [],
   "source": [
    "# Display the decision boundary of the linear model governed by the parameters below.\n",
    "theta_1_random, theta_2_random = -0.5, 1\n",
    "\n",
    "plot_boundary(X_1, y_1, theta_1_random, theta_2_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93491691",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "93491691"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 1.2 (5 points): Loss function\n",
    "\n",
    "In this problem, we apply the mean squared error (MSE), **which is 1/n of the squared error**, as the loss function for gradient decent. MSE is basically the same as the loss function in Lecture 5 other than the scale of the loss function.\n",
    "\n",
    "- Implement the function `MSE`.\n",
    "- Report MSE of the linear model defined by `theta_1_random` and `theta_2_random`, across the dataset ($\\textbf{X}$, $\\textbf{y}$);\n",
    "- Visualize the countour plot of MSE in $\\theta$ space, from $\\theta_1 \\in [-2,2]$ and $\\theta_2 \\in [-2,2]$. (Hint: use `np.meshgrid` to create a grid of possible $(\\theta_1, \\theta_2)$ values. Use `ax.contour` to plot the values of $J$ in every value in the grid.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LEPyzx2IfeA1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LEPyzx2IfeA1"
   },
   "source": [
    "MSE is defined as \n",
    "$$\n",
    "J=\\frac{1}{n} \\sum_{i=1}^n(f(\\textbf{x}_{i})-y_i; \\boldsymbol{\\theta})^2=\\frac{1}{n} \\sum_{i=1}^n(\\theta_1 x_{i1} + \\theta_2 x_{i2}-y_i)^2\n",
    "$$\n",
    "- $n$ is the number of training data\n",
    "- $\\textbf{x}_i$ is the feature vector for row $i$ in a data matrix $\\textbf{X}$, where $i = 1, ..., n$ is an index over the rows and where $x_{ij}$ refers to the value of the j-th component of the vector $\\textbf{x}_i$, where $j = 1, 2$ is an index over features.\n",
    "- $y_i$ is the class label of the feature vector $\\textbf{x}_i$.\n",
    "- $\\theta_1$ and $\\theta_2$ are the parameters of the linear model $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371fe95",
   "metadata": {
    "id": "f371fe95",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def predict(x_1, x_2, theta_1, theta_2):\n",
    "    return theta_1 * x_1 + theta_2 * x_2\n",
    "\n",
    "def MSE(X, y, theta_1, theta_2): \n",
    "    '''\n",
    "    X:       [# data points, # features], feature vectors\n",
    "    y:       [# data points], labels\n",
    "    theta_1: the first parameter of linear model\n",
    "    theta_2: the second parameter of linear model\n",
    "    '''\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    # get the first and second features\n",
    "    ...\n",
    "    \n",
    "    ###  YOUR CODE ENDS HERE  ###\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42933b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The code below visualizes the countour plot of MSE in $\\theta$ space, from $\\theta_1 \\in [-2,2]$ and $\\theta_2 \\in [-2,2]$. It uses `np.meshgrid` to create a grid of possible $(\\theta_1, \\theta_2)$ values. Use `ax.contour` to plot the values of $J$ in every value in the grid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ab81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "62a0ab81"
   },
   "outputs": [],
   "source": [
    "def plot_MSE_contour(X, y):\n",
    "    fig, axes = plt.subplots()\n",
    "\n",
    "    Theta_1, Theta_2 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "    \n",
    "    bs_map = []\n",
    "    for theta_1, theta_2 in zip(Theta_1.ravel(), Theta_2.ravel()):\n",
    "        bs_map.append(MSE(X, y, theta_1, theta_2))\n",
    "        \n",
    "    bs_map = np.asarray(bs_map).reshape(100, 100)\n",
    "    \n",
    "    axes.set_xlabel('theta_1', fontsize=14)\n",
    "    axes.set_ylabel('theta_2', fontsize=14)\n",
    "    axes.set_title('MSE', fontsize=18);\n",
    "\n",
    "    im = axes.contour(Theta_1, Theta_2, bs_map, 100)\n",
    "    fig.colorbar(im, ax=axes);\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204293b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "deletable": false,
    "editable": false,
    "id": "a204293b",
    "otter": {
     "tests": [
      "q1.2 Loss function"
     ]
    },
    "outputId": "27851b35-a73a-47b1-f16d-ada565f033ca"
   },
   "outputs": [],
   "source": [
    "fig, axes = plot_MSE_contour(X_1, y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65331f04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "65331f04"
   },
   "source": [
    "### Problem 1.3 (10 points): Compute the gradient of MSE\n",
    "\n",
    "- Note the gradient of MSE with respect to $\\theta_1$ and $\\theta_2$, i.e., $\\frac{\\partial}{\\partial \\theta_1} L(\\theta_1, \\theta_2)$ and $\\frac{\\partial}{\\partial \\theta_2} L(\\theta_1, \\theta_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275159fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "45ad8a41"
   },
   "source": [
    "The gradient of MSE is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_1} L(\\theta_1, \\theta_2) = \\frac{2}{n} \\sum_{i=1}^n[(\\theta_1 x_{i1} + \\theta_2 x_{i2}-y_i)\\cdot x_{i1}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_2} L(\\theta_1, \\theta_2) = \\frac{2}{n} \\sum_{i=1}^n[(\\theta_1 x_{i1} + \\theta_2 x_{i2}-y_i)\\cdot x_{i2}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ce88a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Using the equations above, implement the function `gradient_mse` that computes the gradient of MSE. This function should output a tuple, where the first value is the partial derivative with respect to $\\theta_1$, and the second values is the partial derivative with respect to $\\theta_2$. The two together make up the gradient vector (for this problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705487e1",
   "metadata": {
    "otter": {
     "tests": [
      "q1.3 Compute the gradient of MSE"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_mse(X, y, theta_1, theta_2):\n",
    "    '''\n",
    "    X:       [# data points, # features], feature vectors\n",
    "    y:       [# data points], labels\n",
    "    theta_1: the first parameter of linear model\n",
    "    theta_2: the second parameter of linear model\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    ...\n",
    "    ###  YOUR CODE ENDS HERE  ###\n",
    "    \n",
    "    return grad_1, grad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1, grad_2 = gradient_mse(X_1, y_1, theta_1_random, theta_2_random)\n",
    "print(f' gradient w.r.t theta_1_random: {grad_1}\\n gradient w.r.t theta_2_random: {grad_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154b26b",
   "metadata": {},
   "source": [
    "**Sample output of** `gradient_mse(X_1, y_1, 1, 0.5)`:\n",
    "$$ \\left.\\frac{\\partial}{\\partial \\theta_{1}} L\\left(\\theta_{1}, \\theta_{2}\\right)\\right|_{\\theta_{1}=1, \\theta_{2}=0.5} = 1.4649435710370162 $$\n",
    "\n",
    "$$ \\left.\\frac{\\partial}{\\partial \\theta_{2}} L\\left(\\theta_{1}, \\theta_{2}\\right)\\right|_{\\theta_{1}=1, \\theta_{2}=0.5} = 1.2124544444898946 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377000ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.4 (10 points): Gradient descent algorithm\n",
    "\n",
    "- Implement the function `gradient_descent`, this function should output a list of `theta_1s`, `theta_2s`, and `mses` containing the value of each of each variable at each step.\n",
    "- Create a plot of MSE vs training iteration, to see if it converges. (hint: use the `mses` list from the last point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3e068",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta_1_init, theta_2_init, lr=0.001, max_iters=100, display_boundary=False):\n",
    "    # use theta_1s and theta_2s to denote the set of updates about theta_1 and theta_2\n",
    "    theta_1s = [theta_1_init]\n",
    "    theta_2s = [theta_2_init]\n",
    "    mses = [MSE(X, y, theta_1_init, theta_2_init)]\n",
    "    \n",
    "    # initialize theta_1 and theta_2 to theta_1_init and theta_2_init\n",
    "    theta_1, theta_2 = theta_1_init, theta_2_init\n",
    "    \n",
    "    # gradient descent iterations\n",
    "    for iter in range(max_iters):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "        # compute the gradient of MSE w.r.t to theta_1 and theta_2 (i.e., grad_1 and grad_2)\n",
    "        grad_1, grad_2 = ...\n",
    "        \n",
    "        # update theta_1 and theta_2 based on the gradients and learning rate (lr)\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "        # compute the MSE based on the updated parameters theta_1 and theta_2\n",
    "        mse = ...\n",
    "        \n",
    "        ###  YOUR CODE ENDS HERE  ###\n",
    "        \n",
    "        theta_1s.append(theta_1) # add new updates about theta_1 to theta_1s\n",
    "        theta_2s.append(theta_2) # add new updates about theta_2 to theta_2s\n",
    "        \n",
    "        mses.append(mse)\n",
    "        \n",
    "        # show the decision boundary every 20 iterations\n",
    "        if iter % 20 == 0 and display_boundary:\n",
    "            plot_boundary(X, y, theta_1, theta_2)\n",
    "\n",
    "    return theta_1s, theta_2s, mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4b618",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run the code below to fit a model using MSE. Don't change the function argument values we provide: \n",
    "# (i.e., the learning rate is 0.05, maximum iteration is 100 and display boundary during gradient descent).\n",
    "theta_1s, theta_2s, mses = gradient_descent(X_1, y_1, theta_1_random, theta_2_random, \n",
    "                                            lr=0.05, max_iters=100, display_boundary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62380f8d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Create a plot of MSE vs training iteration to check that it converges. You can run the code given below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3631839",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create a plot of MSE vs training iteration\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(mses)\n",
    "axes.set_xlabel('Iteration', fontsize=14)\n",
    "axes.set_ylabel('MSE', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6153815",
   "metadata": {
    "deletable": false,
    "editable": false,
    "otter": {
     "tests": [
      "q1.4 Gradient descent algorithm"
     ]
    }
   },
   "source": [
    "**If your implementation of `gradient_descent` is correct, the curve should converge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ce0e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 1.5 (10 points): Plot the trajectory of gradient descent \n",
    "\n",
    "- Using `axes.arrow`, show the position of $\\theta$ at every step, and the corresponding gradient vectors. \n",
    "- Describe in words what you observe from the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df85262",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell and it will display the trajectory of gradient descent\n",
    "# In this problem, the initialization of (theta_1, theta_2) is (2,2). Please don't change any parameter below.\n",
    "theta_1s_demo, theta_2s_demo, _ = gradient_descent(X_1, y_1, 2, 2, lr=0.3, max_iters=10, display_boundary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7ba09-32d9-44d6-af78-b7ace9882b16",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_decent_trajectory(X, y, theta_1s, theta_2s):\n",
    "    fig, axes = plot_MSE_contour(X, y)\n",
    "\n",
    "    theta_1s = np.asarray(theta_1s)\n",
    "    theta_2s = np.asarray(theta_2s)\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "    # use 'axes.arrow' (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.arrow.html)\n",
    "    # to generate one vector with arrow on the fig\n",
    "    # you should call 'axes.arrow' iteratively to generate the whole gradient decent trajectory step by step.\n",
    "    # your job is to identify the 'x, y, dx, dy' for axes.arrow, based on 'theta_1s' and  'theta_2s'\n",
    "    # suggestions of other parameters for 'axes.arrow': head_width = 0.1, color = 'red'\n",
    "\n",
    "    ...\n",
    "    ###  YOUR CODE ENDS HERE  ###\n",
    "    axes.set_xlim(-1,1.5)\n",
    "    axes.set_ylim(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f304c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72abcf1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "gradient_decent_trajectory(X_1, y_1, theta_1s_demo, theta_2s_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4e333",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Describe in words what you observe from the trajectory and explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb150bf",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf3350",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dddf3350"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Problem 2: Logistic Regression\n",
    "\n",
    "<font color='red'><b>Important: We will need to install two more libraries for this assignment, do not skip this.</b></font>\n",
    "\n",
    "We need to run `conda install -c conda-forge simplejson nltk` in order to install `nltk` and `simplejson`. \n",
    "\n",
    "We need `nltk` to work with natural language data.\n",
    "\n",
    "In this problem, you will work with the sklearn implementation of logistic regression on toy data and a more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9965997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import simplejson as json\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884dc46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4884dc46"
   },
   "source": [
    "# Problem 2.1 (15 points): Classify Seeds dataset via scikit-learn implementations of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86049c9-0a65-4f82-aa5e-25c544c51af0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using sklearn's `LogisticRegression` class, train a model (`classifier`) that fits the seeds dataset we have been using from the previous question. Make sure to use the parameters `penalty='none', fit_intercept=False` when instantiating the class. You can view the documentation for this class [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "- Print out the $\\theta_1$ and $\\theta_2$. (Hint: You can get the values of `theta_1_a` and `theta_2_a` from `classifier.coef_`)\n",
    "- Plot the boundaries and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44661d5",
   "metadata": {
    "id": "c44661d5",
    "otter": {
     "tests": [
      "q2.1a Logistic regression with fit_intercept=False"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "# Set up the classifier.\n",
    "clf_a = ...\n",
    "\n",
    "# Get classifier parameters (clf.coef is a 2d-array with shape (2,1)).\n",
    "theta_1_a, theta_2_a = ...\n",
    "\n",
    "###  YOUR CODE ENDS HERE  ###\n",
    "\n",
    "print(theta_1_a, theta_2_a)\n",
    "plot_boundary(X_1, y_1, theta_1_a, theta_2_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544e479",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using sklearn's `LogisticRegression` class, train a model that fits the seeds dataset we have been using from the previous question. Use the parameters `penalty='none', fit_intercept=True` when instantiating the class.\n",
    "\n",
    "- Print out the `theta_0_b`, `theta_1_b` and `theta_2_b`. (Hint: You can get the values of `theta_0_b` from `clf.intercept_`)\n",
    "- You should find $\\theta_0$ is not zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b68cd",
   "metadata": {
    "otter": {
     "tests": [
      "q2.1b Logistic regression with fit_intercept=True"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "\n",
    "# set up the classifier\n",
    "clf_b = ...\n",
    "\n",
    "# get classifier parameters\n",
    "theta_0_b = ...\n",
    "theta_1_b, theta_2_b = ...\n",
    "\n",
    "### YOUR CODE ENDS HERE ###\n",
    "\n",
    "print(theta_0_b, theta_1_b, theta_2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2998aac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "- Implement function `plot_boundary_w_intercept`. Think about how to modify `plot_boundary` to make it compatible with the linear model with bias (i.e., $\\theta_0$ is not necessarily zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf16a8",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_boundary_w_intercept(X, y, theta_0, theta_1, theta_2): \n",
    "    '''\n",
    "    X:       [# feature vectors, # features], feature vectors\n",
    "    y:       [# feature vectors], labels\n",
    "    theta_0: the bias of linear model\n",
    "    theta_1: the first parameter of linear model\n",
    "    theta_2: the second parameter of linear model\n",
    "    '''\n",
    "    \n",
    "    # the code is similar with that in Problem 1, but we have new theta_1 and theta_2 which are the coefficients for the well trained model\n",
    "    fig, axes = plot_data(X, y)\n",
    "    ### YOUR CODE STARTS HERE ##\n",
    "    ...\n",
    "    ### YOUR CODE STARTS HERE ##\n",
    "\n",
    "# Call your function on the classifier you fit in the previous point\n",
    "plot_boundary_w_intercept(X_1, y_1, theta_0_b, theta_1_b, theta_2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43848d15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "43848d15"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 2.2 (10 points): Explore Amazon Review dataset\n",
    "\n",
    "The Amazon Reviews dataset consists of reviews with their scores (1 to 5). This review data can be used in a classification task, where we are given a review and we predict whether it is a positive or a negative review. We will consider a review as *negative* if it has a score $\\leq 2$, or *positive* if it has a score $\\geq 4$.\n",
    "\n",
    "Below we pre-process the dataset for you using what's known as a bag-of-words approach. You will run the code create a bag of words (BOW) representation from text documents, using the Vectorizer function in scikit-learn to convert each text review (a string) to a fixed-length vector of numbers that we use as a feature vector for a classifier. \n",
    "\n",
    "Note that the details of how reviews are converted to vectors is not particularly important for you to follow in detail. What is important is to be aware that after we run the code below we will get a data matrix with n rows (one per review) and d features (where each of the d features corresponds to a different word). Each row in the data matrix is a feature vector for a Amazon review, where the values in the feature vector correspond to how often different words occur in the review.\n",
    "\n",
    "In a bit more detail, the bag of words representation of text is a way to turn text (in the form of a string) into a vector of some fixed dimension. More specifically, every document (e.g., a review) is converted into a vector $\\mathbf{x}$, where $\\mathbf{x}_j$ denotes whether the j-th word from the vocabulary list is used in the sentence. For example, if our vocabulary is `[he, she, his, her, loves, likes, dogs, cats, cute, nice]`, the sentence `\"he loves cats and she loves dogs\"` can be vectorized as `[1, 1, 0, 0, 2, 0, 1, 1, 0, 0]`. This representation allows us to compare documents (e.g., reviews) in the same way we have been doing with tabular data and images.\n",
    "\n",
    "The inputs are \n",
    "- a filename (you will use \"amazon_reviews.csv\") containing the reviews in csv format (provided in your student .zip file)\n",
    "- the min_pos and max_neg parameters (use the default values.)\n",
    "\n",
    "The outputs are\n",
    "- **X**: Feature Matrix in compressed sparse row format.\n",
    "    - $\\textbf{X}$ is in shape of [# reviews, # terms in vocabulary]  (where #terms = #features)\n",
    "    - $\\textbf{X}_{ij}$ indicates the number of the j-th term in the vocabulary existing in the $i$th review.\n",
    "- **y**: Review label vector\n",
    "- **text**: Raw reviews\n",
    "- **vectorizer_BOW.vocabulary_**: A mapping of terms to feature indices, somes terms are excluded\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: \n",
    "- please read the scikit-learn tutorial on text feature extraction before you start this problem: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  (it is fine to just read Sections 6.2.3.1 to 6.2.3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c582f5",
   "metadata": {
    "id": "d3c582f5"
   },
   "outputs": [],
   "source": [
    "def create_bow_from_reviews(filename, min_pos=4, max_neg=2): \n",
    "    \n",
    "    print('Loading the file:', filename) \n",
    "    with open(filename, 'r') as file:\n",
    "        data = pd.read_csv(file)\n",
    "\n",
    "    data_sub = data[['reviewText', 'overall']]\n",
    "    data_clean = data_sub.dropna()\n",
    "        \n",
    "    print('Total number of reviews extracted =', len(data))\n",
    "\n",
    "    print('Extracting tokens from each review.....(can be slow for a large number of reviews)......')   \n",
    "    # do not consider reviews with scores above max_neg and below min_pos (these reviews will be dropped)\n",
    "\n",
    "    text = data_clean['reviewText'].tolist() # keep only the text and label\n",
    "    \n",
    "    # represent scores > min_pos as \"1\"\n",
    "    # represent scores < max_neg as \"0\"\n",
    "    y = data_clean[['overall']].astype(int)\n",
    "    y[y >= min_pos] = 1\n",
    "    y[y <= max_neg] = 0\n",
    "    \n",
    "    # create an instance of a CountVectorizer, using \n",
    "    # (1) the standard 'english' stopword set \n",
    "    # (2) only keeping terms in the vocabulary that occur in at least 1% of documents\n",
    "    # (3) allowing only unigrams in the vocabulary (use \"ngram_range=(1, 1)\" to do this)\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=0.02, ngram_range=(1, 1))\n",
    "\n",
    "    # create a sparse BOW array from 'text' using vectorizer\n",
    "    X = vectorizer.fit_transform(text)\n",
    " \n",
    "    print('Data shape: ', X.shape)\n",
    "    \n",
    "    # you can uncomment this next line if you want to see the full list of tokens in the vocabulary  \n",
    "    #print('Vocabulary: ', vectorizer.get_feature_names())\n",
    " \n",
    "    return X, y, vectorizer, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b1f34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "b58b1f34",
    "outputId": "6d938746-65e9-481a-92f0-e8c18e2dc7af"
   },
   "outputs": [],
   "source": [
    "X_2, y_2, vectorizer_BOW, text = create_bow_from_reviews('data/amazon_reviews.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a1dd4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Plot a histogram of how many unique words are in each review\n",
    "- Implement the function `review_word_count_histogram`, which generates a histogram to show how many unique words (from the vocabulary) are in each review. That is, for each review, count the number of unique words in the review, and plot a histogram illustrating these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b6b2c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def review_word_count_histgram(X):\n",
    "    '''\n",
    "    X:  [# feature vectors, # features], feature vectors\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    ...\n",
    "    ### YOUR CODE ENDS HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136fe8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "review_word_count_histgram(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9eb2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "45d9eb2f"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 2.3 (10 points): Fit a logistic regression classifier on Amazon Review dataset.\n",
    "\n",
    "You will now build a logistic regression model to classify reviews as either negative or positive. In the code you are given, a training/testing split has already been created for you.\n",
    "\n",
    "- Build a logistic classifier on the training subset. **Please use `penalty='l1'`, `solver='liblinear'` and `fit_intercept=True` when calling `LogisticRegression`.**\n",
    "- Evaluate the accuracy of your model on the test subset.\n",
    "\n",
    "**NOTE**: \n",
    "before starting this problem please read the scikit-learn documentation on [linear classifiers](https://scikit-learn.org/stable/modules/linear_model.html) and [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf32bb7",
   "metadata": {
    "id": "9bf32bb7",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def logistic_classification(X, y): \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "    #  set the state of the random number generator so that we get the same results across runs when testing our code\n",
    "     \n",
    "    print('Number of training examples: ', X_train.shape[0])\n",
    "    print('Number of testing examples: ', X_test.shape[0])   \n",
    "    print('Vocabulary size: ', X_train.shape[1]) \n",
    "\n",
    "    ### YOUR CODE START HERE ### \n",
    "\n",
    "    # Specify the logistic classifier model\n",
    "    # # Please use 'l1' penality type, 'liblinear' solver and enable fit_intercept\n",
    "    classifier = ...\n",
    "\n",
    "    # Train a logistic regression classifier and evaluate accuracy on the training data\n",
    "    print('\\nTraining a model with', X_train.shape[0], 'examples.....')\n",
    "    # Training \n",
    "    ...\n",
    "    \n",
    "    train_predictions = ...\n",
    "    train_accuracy = ...\n",
    "    print('\\nTraining accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "\n",
    "    # Compute and print accuracy on the test data\n",
    "    test_predictions = ...\n",
    "    test_accuracy = ...\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    print('\\nTesting accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\n",
    "    return classifier, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e37266",
   "metadata": {
    "deletable": false,
    "editable": false,
    "otter": {
     "tests": [
      "q2.3 Fit a logistic regression classifier on Amazon Reviews"
     ]
    }
   },
   "outputs": [],
   "source": [
    "logistic_classifier, _, _ = logistic_classification(X_2, y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e8338",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 2.4 (10 points): Find the top k most negative and most positive features\n",
    "\n",
    "In this problem, you will complete the function `most_significant_terms` to \n",
    "- print out and return the most significant positive and negative weights \n",
    "- print our the associated terms\n",
    "\n",
    "`most_significant_terms` takes as input\n",
    "- a scikit-learn trained logistic regression classifier (e.g., trained in Problem 2.3) \n",
    "- a scikit-learn vectorizer object that produced the BOW features for the classifier\n",
    "\n",
    "and prints out\n",
    "- the terms in the vocabulary tokens with the **top10** largest positive weights  \n",
    "- the terms in the vocabulary with the **top10** smallest (i.e., largest in terms of absolute value) negative weights\n",
    "\n",
    "Hint: Check the attributes section of the vectorizer documentation (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to find where the mapping between words and indices are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c9360",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "    # cycle through the positive weights, in the order of largest weight first and print out\n",
    "    # K lines where each line contains\n",
    "    # (a) the term corresponding to the weight (a string)\n",
    "    # (b) the weight value itself (a scalar printed to 3 decimal places)\n",
    "\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    # extract coefficients from trained model and sort based on coefficient\n",
    "    feature_importance = ...\n",
    "    # the index of sorted coefficient\n",
    "    sorted_idx = ...\n",
    "    # extract topK positive terms\n",
    "    topK_pos_terms = [list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(w)] for w in\n",
    "                      ...\n",
    "    # extract corresponding weights\n",
    "    topK_pos_weights = ...\n",
    "\n",
    "    topK_neg_terms = [list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(w)] for w in\n",
    "                      ...\n",
    "    topK_neg_weights = ...\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    print('topK_pos_weights', topK_pos_weights)\n",
    "    print('topK_pos_terms', topK_pos_terms)\n",
    "    print('topK_neg_weights', topK_neg_weights)\n",
    "    print('topK_neg_terms',topK_neg_terms)\n",
    "    return topK_pos_weights, topK_pos_terms, topK_neg_weights, topK_neg_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be7508",
   "metadata": {
    "deletable": false,
    "editable": false,
    "otter": {
     "tests": [
      "q2.4 Find top k most negative and positive features"
     ]
    }
   },
   "outputs": [],
   "source": [
    "topK_pos_weights, topK_pos_terms, topK_neg_weights, topK_neg_terms = most_significant_terms(logistic_classifier, vectorizer_BOW, K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f502f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 2.5 (10 points): Tune regularization parameter on Amazon Reviews dataset\n",
    "\n",
    "In this problem, you will tune the regularization parameter of your logistic regression model in order to increase your accuracy.\n",
    "\n",
    "- Complete the function `accuracy_vs_reg`. This function should perform L1 regularization using the values defined in the array `cs` below. For each value of the regularization parameter in `cs`, you should store the resulting training accuracy in the list `train_accuracies` and the resulting testing accuracy in the list `test_accuracies`.\n",
    "- Use your implementation to plot the curves of train/test accuracy vs regularization strength, for the Amazon review data we used above. The plotting code necessary to do this is already given to you, and you only need to call the function.\n",
    "\n",
    "When plotting the accuracy against the regularization strength we notice that sci-kit learn's implementation of logistic regression [linked here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) uses the inverse of regularization strength as a positive float. Smaller values specify stronger regularization which is why we invert the x-axis in the plotting code provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bd6fb",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def accuracy_vs_reg(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "   \n",
    "    # log initialization\n",
    "    coefs_ = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    cs = [0, 0.1, 1, 10, 50]\n",
    "\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    for c in cs:\n",
    "        # Specify the logistic classifier model\n",
    "        if c == 0:\n",
    "            classifier = LogisticRegression(penalty=None, fit_intercept=True)\n",
    "        else:\n",
    "            # Refer https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "            # to see which argument can adjust the regularization strength,\n",
    "            # Additionally, please use 'l1' penalty type, 'liblinear' solver and enable fit_intercept\n",
    "            classifier = ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        # log model coefficients\n",
    "        ...\n",
    "\n",
    "        # Training\n",
    "        train_predictions = ...\n",
    "        train_accuracy = ...\n",
    "        ...\n",
    "        \n",
    "        # Testing: compute and print accuracy and AUC on the test data\n",
    "        test_predictions = ...\n",
    "        test_accuracy = ...\n",
    "        ...\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.xaxis.set_inverted(True)\n",
    "    axes.semilogx(cs, train_accuracies, color='red', label='training accuracy')\n",
    "    axes.semilogx(cs, test_accuracies, color='blue', label='testing accuracy')\n",
    "    \n",
    "    axes.set_xlabel('regularization strength', fontsize=14)\n",
    "    axes.set_ylabel('accuracy', fontsize=14)\n",
    "    \n",
    "    axes.legend()\n",
    "\n",
    "    return train_accuracies, test_accuracies, coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469e811-bf04-4496-ac6d-8144e4de5db9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_accuracies, test_accuracies, coefs = accuracy_vs_reg(X_2, y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a052765",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4a052765"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Statement of Collaboration (5 points)\n",
    "\n",
    "It is **mandatory** to include a Statement of Collaboration in each submission, with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
    "\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments, in particular, I encourage the students to organize (perhaps using EdD) to\n",
    "discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start\n",
    "working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not\n",
    "allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard,\n",
    "written notes, referring to EdD, etc.). Especially after you have started working on the assignment, try\n",
    "to restrict the discussion to EdD as much as possible, so that there is no doubt as to the extent of your\n",
    "collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e1c7f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f24018",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "otter": {
   "OK_FORMAT": false,
   "assignment_name": "hw2",
   "tests": {
    "q1.2 Loss function": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q1.2 Loss function\"\npoints = 5\n\n",
    "q1.3 Compute the gradient of MSE": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q1.3 Compute the gradient of MSE\"\npoints = 10\n\n",
    "q1.4 Gradient descent algorithm": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q1.4 Gradient descent algorithm\"\npoints = 10\n\n",
    "q2.1a Logistic regression with fit_intercept=False": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.1a Logistic regression with fit_intercept=False\"\npoints = 5\n\n",
    "q2.1b Logistic regression with fit_intercept=True": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.1b Logistic regression with fit_intercept=True\"\npoints = 5\n\n",
    "q2.3 Fit a logistic regression classifier on Amazon Reviews": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.3 Fit a logistic regression classifier on Amazon Reviews\"\npoints = 10\n\n",
    "q2.4 Find top k most negative and positive features": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.4 Find top k most negative and positive features\"\npoints = 10\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
